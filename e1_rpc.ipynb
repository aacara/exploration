{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regulated-disaster",
   "metadata": {},
   "source": [
    "## Exploration_e-1 가위바위보 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "intensive-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "foster-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499  images to be resized.\n",
      "499  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "500  images to be resized.\n",
      "500  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "500  images to be resized.\n",
      "500  images resized.\n",
      "보 이미지 resize 완료!\n"
     ]
    }
   ],
   "source": [
    "def resize_images(img_path):\n",
    "    images=glob.glob(img_path + \"/*.jpg\")  \n",
    "\n",
    "    print(len(images), \" images to be resized.\")\n",
    "\n",
    "    # 파일마다 모두 28x28 사이즈로 바꾸어 저장합니다.\n",
    "    target_size=(28,28)\n",
    "    for img in images:\n",
    "        old_img=Image.open(img)\n",
    "        new_img=old_img.resize(target_size,Image.ANTIALIAS)\n",
    "        new_img.save(img, \"JPEG\")\n",
    "    \n",
    "    print(len(images), \" images resized.\")\n",
    "    \n",
    "# 가위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/scissor\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"가위 이미지 resize 완료!\")\n",
    "\n",
    "# 바위 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/rock\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"바위 이미지 resize 완료!\")\n",
    "\n",
    "# 보 이미지가 저장된 디렉토리 아래의 모든 jpg 파일을 읽어들여서\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper/paper\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"보 이미지 resize 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "consolidated-contents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터(x_train)의 이미지 개수는 1499 입니다.\n",
      "x_train shape: (1500, 28, 28, 3)\n",
      "y_train shape: (1500,)\n",
      "라벨:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(img_path, number_of_data=1500):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor/*.jpg'): #for file in glob.iglob(img_path+'/scissor_s/*.jpg'): \n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock/*.jpg'): #for file in glob.iglob(img_path+'/rock_s/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper/*.jpg'):#for file in glob.iglob(img_path+'/paper_s/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/rock_scissor_paper\"\n",
    "(x_train, y_train) = load_data(image_dir_path)\n",
    "x_train_norm = x_train/255.0\n",
    "\n",
    "\n",
    "print(\"x_train shape: {}\".format(x_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "print('라벨: ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "sublime-marking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 26, 26, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 18,074\n",
      "Trainable params: 18,074\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_channel_1=16\n",
    "n_channel_2=32\n",
    "n_dense=16\n",
    "n_train_epoch=50\n",
    "\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(n_channel_1, (3,3), activation='relu', input_shape=(28,28,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(n_channel_2, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(n_dense, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "rural-indian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.5416 - accuracy: 0.3071\n",
      "Epoch 2/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.1060 - accuracy: 0.3682\n",
      "Epoch 3/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.0538 - accuracy: 0.4402\n",
      "Epoch 4/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 1.0029 - accuracy: 0.4959\n",
      "Epoch 5/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.9648 - accuracy: 0.4892\n",
      "Epoch 6/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.9007 - accuracy: 0.6207\n",
      "Epoch 7/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.7927 - accuracy: 0.6835\n",
      "Epoch 8/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.6590 - accuracy: 0.7473\n",
      "Epoch 9/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.5492 - accuracy: 0.7961\n",
      "Epoch 10/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.4655 - accuracy: 0.8412\n",
      "Epoch 11/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.8490\n",
      "Epoch 12/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8823\n",
      "Epoch 13/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.3006 - accuracy: 0.9002\n",
      "Epoch 14/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.3099 - accuracy: 0.8820\n",
      "Epoch 15/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.2932 - accuracy: 0.8909\n",
      "Epoch 16/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.3152 - accuracy: 0.8643\n",
      "Epoch 17/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9253\n",
      "Epoch 18/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.2067 - accuracy: 0.9443\n",
      "Epoch 19/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9451\n",
      "Epoch 20/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1618 - accuracy: 0.9562\n",
      "Epoch 21/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1502 - accuracy: 0.9633\n",
      "Epoch 22/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1507 - accuracy: 0.9434\n",
      "Epoch 23/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1229 - accuracy: 0.9683\n",
      "Epoch 24/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1318 - accuracy: 0.9630\n",
      "Epoch 25/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1185 - accuracy: 0.9646\n",
      "Epoch 26/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0943 - accuracy: 0.9760\n",
      "Epoch 27/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1041 - accuracy: 0.9684\n",
      "Epoch 28/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9734\n",
      "Epoch 29/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9806\n",
      "Epoch 30/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9820\n",
      "Epoch 31/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9903\n",
      "Epoch 32/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9592\n",
      "Epoch 33/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9898\n",
      "Epoch 34/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9826\n",
      "Epoch 35/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.9834\n",
      "Epoch 36/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0501 - accuracy: 0.9952\n",
      "Epoch 37/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0410 - accuracy: 0.9982\n",
      "Epoch 38/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.9934\n",
      "Epoch 39/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0500 - accuracy: 0.9859\n",
      "Epoch 40/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.9865\n",
      "Epoch 41/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.9960\n",
      "Epoch 42/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9969\n",
      "Epoch 43/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9976\n",
      "Epoch 44/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9983\n",
      "Epoch 45/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9982\n",
      "Epoch 46/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.9991\n",
      "Epoch 47/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.9991\n",
      "Epoch 48/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.9966\n",
      "Epoch 49/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0398 - accuracy: 0.9877\n",
      "Epoch 50/50\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f08402c6e90>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(x_train_norm, y_train, epochs=n_train_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "speaking-federal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500  images to be resized.\n",
      "500  images resized.\n",
      "가위 이미지 resize 완료!\n",
      "500  images to be resized.\n",
      "500  images resized.\n",
      "바위 이미지 resize 완료!\n",
      "500  images to be resized.\n",
      "500  images resized.\n",
      "보 이미지 resize 완료!\n",
      "트레이닝데이터(x_train)의 이미지 개수는 1500 입니다.\n"
     ]
    }
   ],
   "source": [
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/test/scissor_s\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"가위 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/test/rock_s\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"바위 이미지 resize 완료!\")\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/test/paper_s\"\n",
    "resize_images(image_dir_path)\n",
    "print(\"보 이미지 resize 완료!\")\n",
    "\n",
    "def load_data(img_path, number_of_data=1500):  # 가위바위보 이미지 개수 총합에 주의하세요.\n",
    "    # 가위 : 0, 바위 : 1, 보 : 2\n",
    "    img_size=28\n",
    "    color=3\n",
    "    #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다.\n",
    "    imgs=np.zeros(number_of_data*img_size*img_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color)\n",
    "    labels=np.zeros(number_of_data,dtype=np.int32)\n",
    "\n",
    "    idx=0\n",
    "    for file in glob.iglob(img_path+'/scissor_s/*.jpg'): \n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=0   # 가위 : 0\n",
    "        idx=idx+1\n",
    "\n",
    "    for file in glob.iglob(img_path+'/rock_s/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=1   # 바위 : 1\n",
    "        idx=idx+1  \n",
    "    \n",
    "    for file in glob.iglob(img_path+'/paper_s/*.jpg'):\n",
    "        img = np.array(Image.open(file),dtype=np.int32)\n",
    "        imgs[idx,:,:,:]=img    # 데이터 영역에 이미지 행렬을 복사\n",
    "        labels[idx]=2   # 보 : 2\n",
    "        idx=idx+1\n",
    "        \n",
    "    print(\"트레이닝데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")\n",
    "    return imgs, labels\n",
    "\n",
    "image_dir_path = os.getenv(\"HOME\") + \"/aiffel/test\"\n",
    "(x_test, y_test) = load_data(image_dir_path)\n",
    "x_test_norm = x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-spider",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "placed-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 - 0s - loss: 264.6937 - accuracy: 0.6827\n",
      "loss : 264.6936950683594\n",
      "accuracy : 0.6826666593551636\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose = 2)\n",
    "print(\"loss : {}\".format(test_loss))\n",
    "print(\"accuracy : {}\".format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
